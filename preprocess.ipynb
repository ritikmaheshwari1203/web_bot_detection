{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Regex pattern for your specific log format\n",
    "# -------------------------------------------------------------------\n",
    "# A line example is:\n",
    "# - - [29/Oct/2019:09:18:00 +0000] \"POST /storage/store_sess_total_mousemv_db.php HTTP/1.1\" 200 449 \"https://160.40.52.164/\" g2gh9qmk9krld14h5uojlg7g10 \"Mozilla/5.0 ...\"\n",
    "#\n",
    "# Explanation of groups:\n",
    "#   (?P<ident1>\\S+)     -> The first '-' (could be IP if you have logs with IP)\n",
    "#   (?P<ident2>\\S+)     -> The second '-'\n",
    "#   \\[(?P<time>[^\\]]+)\\]-> [29/Oct/2019:09:18:00 +0000]\n",
    "#   \"(?P<method>\\S+)    -> \"POST\n",
    "#   (?P<endpoint>\\S+)   -> /storage/store_sess_total_mousemv_db.php\n",
    "#   (?P<protocol>[^\"]+)\"-> HTTP/1.1\"\n",
    "#   (?P<status>\\d+)     -> 200\n",
    "#   (?P<size>\\d+)       -> 449\n",
    "#   \"(?P<referrer>[^\"]*)\"-> \"https://160.40.52.164/\"\n",
    "#   (?P<session_id>\\S+) -> g2gh9qmk9krld14h5uojlg7g10\n",
    "#   \"(?P<user_agent>[^\"]*)\"-> \"Mozilla/5.0 ...\"\n",
    "#\n",
    "# We anchor with ^ and $ so it must match the full line.\n",
    "# If your log lines differ, you'll need to adjust this pattern.\n",
    "# -------------------------------------------------------------------\n",
    "log_pattern = re.compile(\n",
    "    r'^'\n",
    "    r'(?P<ident1>\\S+)\\s+'                     # e.g. '-'\n",
    "    r'(?P<ident2>\\S+)\\s+'                     # e.g. '-'\n",
    "    r'\\[(?P<time>[^\\]]+)\\]\\s+'                # [29/Oct/2019:09:18:00 +0000]\n",
    "    r'\"(?P<method>\\S+)\\s+(?P<endpoint>\\S+)\\s+(?P<protocol>[^\"]+)\"\\s+'  # \"POST /storage/... HTTP/1.1\"\n",
    "    r'(?P<status>\\d+)\\s+'                     # 200\n",
    "    r'(?P<size>\\d+)\\s+'                       # 449\n",
    "    r'\"(?P<referrer>[^\"]*)\"\\s+'               # \"https://160.40.52.164/\"\n",
    "    r'(?P<session_id>\\S+)\\s+'                 # g2gh9qmk9krld14h5uojlg7g10\n",
    "    r'\"(?P<user_agent>[^\"]*)\"'                # \"Mozilla/5.0 ...\"\n",
    "    r'$'\n",
    ")\n",
    "\n",
    "def parse_apache_line(line):\n",
    "    \"\"\"\n",
    "    Parse a single log line using the regex above.\n",
    "    Return a dictionary with named fields or None if the line doesn't match.\n",
    "    \"\"\"\n",
    "    match = log_pattern.search(line)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    data = match.groupdict()\n",
    "    \n",
    "    # Convert the time string to a datetime object\n",
    "    # Example: 29/Oct/2019:09:18:00 +0000\n",
    "    time_str = data[\"time\"]\n",
    "    try:\n",
    "        # Using %z to parse the +0000 timezone\n",
    "        dt_obj = datetime.strptime(time_str, \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "        data[\"time\"] = dt_obj\n",
    "    except ValueError:\n",
    "        # If parsing fails, store None\n",
    "        data[\"time\"] = None\n",
    "    \n",
    "    # Convert status and size to integer\n",
    "    data[\"status\"] = int(data[\"status\"])\n",
    "    data[\"size\"] = int(data[\"size\"])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_features_from_logs(log_file_path):\n",
    "    \"\"\"\n",
    "    Reads a log file in the given format, groups lines by session_id,\n",
    "    and computes a few example features:\n",
    "      - total_requests\n",
    "      - total_session_bytes\n",
    "      - number of GET/POST requests\n",
    "      - time_spent (approx. difference between first and last request)\n",
    "      - average inter-request time\n",
    "    Returns a pandas DataFrame with one row per session.\n",
    "    \"\"\"\n",
    "    sessions = {}\n",
    "    \n",
    "    # 1) Read the log file and parse each line\n",
    "    with open(log_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parsed = parse_apache_line(line)\n",
    "            if parsed is None:\n",
    "                # Could not parse; skip or handle differently\n",
    "                continue\n",
    "            \n",
    "            sess_id = parsed[\"session_id\"]\n",
    "            if sess_id not in sessions:\n",
    "                sessions[sess_id] = {\n",
    "                    \"entries\": []\n",
    "                }\n",
    "            sessions[sess_id][\"entries\"].append(parsed)\n",
    "    \n",
    "    # 2) Compute features per session\n",
    "    rows = []\n",
    "    for sess_id, sess_data in sessions.items():\n",
    "        entries = sess_data[\"entries\"]\n",
    "        if not entries:\n",
    "            continue\n",
    "        \n",
    "        # Sort by time to compute durations & inter-request intervals\n",
    "        entries.sort(key=lambda x: x[\"time\"] if x[\"time\"] else datetime.min)\n",
    "        \n",
    "        # --- Feature: total requests\n",
    "        total_requests = len(entries)\n",
    "        \n",
    "        # --- Feature: total bytes\n",
    "        total_session_bytes = sum(e[\"size\"] for e in entries)\n",
    "        \n",
    "        # --- Feature: method counts\n",
    "        method_counts = {}\n",
    "        for e in entries:\n",
    "            m = e[\"method\"]\n",
    "            method_counts[m] = method_counts.get(m, 0) + 1\n",
    "        num_get = method_counts.get(\"GET\", 0)\n",
    "        num_post = method_counts.get(\"POST\", 0)\n",
    "        \n",
    "        # --- Feature: time spent (approx)\n",
    "        # Difference between first and last request timestamps\n",
    "        time_spent = 0\n",
    "        if entries[0][\"time\"] and entries[-1][\"time\"]:\n",
    "            time_spent = (entries[-1][\"time\"] - entries[0][\"time\"]).total_seconds()\n",
    "        \n",
    "        # --- Feature: average inter-request time\n",
    "        inter_times = []\n",
    "        for i in range(1, len(entries)):\n",
    "            t1 = entries[i-1][\"time\"]\n",
    "            t2 = entries[i][\"time\"]\n",
    "            if t1 and t2:\n",
    "                diff = (t2 - t1).total_seconds()\n",
    "                inter_times.append(diff)\n",
    "        \n",
    "        if inter_times:\n",
    "            avg_inter_request = sum(inter_times) / len(inter_times)\n",
    "        else:\n",
    "            avg_inter_request = 0\n",
    "        \n",
    "        row = {\n",
    "            \"session_id\": sess_id,\n",
    "            \"total_requests\": total_requests,\n",
    "            \"total_session_bytes\": total_session_bytes,\n",
    "            \"num_get\": num_get,\n",
    "            \"num_post\": num_post,\n",
    "            \"time_spent\": time_spent,\n",
    "            \"avg_inter_request\": avg_inter_request,\n",
    "        }\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # 3) Create a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your actual log file\n",
    "    log_file = \"web_bot_detection_dataset/phase1/data/web_logs/bots/access_advanced_bots.log\"\n",
    "    df_features = extract_features_from_logs(log_file)\n",
    "    print(df_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Regex for parsing the log lines\n",
    "log_pattern = re.compile(\n",
    "    r'^'\n",
    "    r'(?P<ident1>\\S+)\\s+'\n",
    "    r'(?P<ident2>\\S+)\\s+'\n",
    "    r'\\[(?P<time>[^\\]]+)\\]\\s+'\n",
    "    r'\"(?P<method>\\S+)\\s+(?P<endpoint>\\S+)\\s+(?P<protocol>[^\"]+)\"\\s+'\n",
    "    r'(?P<status>\\d+)\\s+'\n",
    "    r'(?P<size>\\d+)\\s+'\n",
    "    r'\"(?P<referrer>[^\"]*)\"\\s+'\n",
    "    r'(?P<session_id>\\S+)\\s+'\n",
    "    r'\"(?P<user_agent>[^\"]*)\"'\n",
    "    r'$'\n",
    ")\n",
    "\n",
    "def parse_log_line(line):\n",
    "    match = log_pattern.search(line)\n",
    "    if not match:\n",
    "        return None\n",
    "    data = match.groupdict()\n",
    "    time_str = data[\"time\"]\n",
    "    try:\n",
    "        dt_obj = datetime.strptime(time_str, \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "        data[\"time\"] = dt_obj\n",
    "    except ValueError:\n",
    "        data[\"time\"] = None\n",
    "    data[\"status\"] = int(data[\"status\"])\n",
    "    data[\"size\"] = int(data[\"size\"])\n",
    "    return data\n",
    "\n",
    "def extract_log_features_from_dir(logs_dir):\n",
    "    sessions = {}\n",
    "    # Iterate over all log files in the given directory\n",
    "    for filename in os.listdir(logs_dir):\n",
    "        file_path = os.path.join(logs_dir, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parsed = parse_log_line(line)\n",
    "                if parsed is None:\n",
    "                    continue\n",
    "                sid = parsed[\"session_id\"]\n",
    "                if sid not in sessions:\n",
    "                    sessions[sid] = {\"entries\": []}\n",
    "                sessions[sid][\"entries\"].append(parsed)\n",
    "    return sessions\n",
    "\n",
    "def extract_log_features(phase_dir):\n",
    "    sessions = {}\n",
    "    # Phase directory contains \"data/web_logs\" with subdirectories \"human\" and \"bots\"\n",
    "    human_logs_dir = os.path.join(phase_dir, \"data\", \"web_logs\", \"human\")\n",
    "    bot_logs_dir = os.path.join(phase_dir, \"data\", \"web_logs\", \"bots\")\n",
    "    # Process human logs\n",
    "    if os.path.exists(human_logs_dir):\n",
    "        human_sessions = extract_log_features_from_dir(human_logs_dir)\n",
    "        sessions.update(human_sessions)\n",
    "    # Process bot logs\n",
    "    if os.path.exists(bot_logs_dir):\n",
    "        bot_sessions = extract_log_features_from_dir(bot_logs_dir)\n",
    "        for sid, data in bot_sessions.items():\n",
    "            if sid in sessions:\n",
    "                sessions[sid][\"entries\"].extend(data[\"entries\"])\n",
    "            else:\n",
    "                sessions[sid] = data\n",
    "    rows = []\n",
    "    for sid, sess_data in sessions.items():\n",
    "        entries = sess_data[\"entries\"]\n",
    "        if not entries:\n",
    "            continue\n",
    "        entries.sort(key=lambda x: x[\"time\"] if x[\"time\"] else datetime.min)\n",
    "        total_requests = len(entries)\n",
    "        total_session_bytes = sum(e[\"size\"] for e in entries)\n",
    "        method_counts = {}\n",
    "        for e in entries:\n",
    "            m = e[\"method\"]\n",
    "            method_counts[m] = method_counts.get(m, 0) + 1\n",
    "        num_get = method_counts.get(\"GET\", 0)\n",
    "        num_post = method_counts.get(\"POST\", 0)\n",
    "        time_spent = 0\n",
    "        if entries[0][\"time\"] and entries[-1][\"time\"]:\n",
    "            time_spent = (entries[-1][\"time\"] - entries[0][\"time\"]).total_seconds()\n",
    "        inter_times = []\n",
    "        for i in range(1, len(entries)):\n",
    "            t1 = entries[i-1][\"time\"]\n",
    "            t2 = entries[i][\"time\"]\n",
    "            if t1 and t2:\n",
    "                inter_times.append((t2 - t1).total_seconds())\n",
    "        avg_inter_request = sum(inter_times) / len(inter_times) if inter_times else 0\n",
    "        row = {\n",
    "            \"session_id\": sid,\n",
    "            \"total_requests\": total_requests,\n",
    "            \"total_session_bytes\": total_session_bytes,\n",
    "            \"num_get\": num_get,\n",
    "            \"num_post\": num_post,\n",
    "            \"time_spent\": time_spent,\n",
    "            \"avg_inter_request\": avg_inter_request,\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def extract_mouse_features(mouse_root):\n",
    "    all_features = []\n",
    "    # mouse_root is a folder containing subdirectories for each session\n",
    "    for session_id in os.listdir(mouse_root):\n",
    "        session_path = os.path.join(mouse_root, session_id)\n",
    "        if not os.path.isdir(session_path):\n",
    "            continue\n",
    "        mouse_json = os.path.join(session_path, \"mouse_movements.json\")\n",
    "        if not os.path.exists(mouse_json):\n",
    "            continue\n",
    "        with open(mouse_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except:\n",
    "                continue\n",
    "        coords = data.get(\"mousemove_total_behaviour\", [])\n",
    "        num_moves = len(coords)\n",
    "        avg_x, avg_y = 0, 0\n",
    "        if coords and isinstance(coords[0], list) and len(coords[0]) == 2:\n",
    "            xs = [c[0] for c in coords]\n",
    "            ys = [c[1] for c in coords]\n",
    "            avg_x = np.mean(xs)\n",
    "            avg_y = np.mean(ys)\n",
    "        all_features.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"num_moves\": num_moves,\n",
    "            \"avg_x\": avg_x,\n",
    "            \"avg_y\": avg_y\n",
    "        })\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "def merge_features(df_logs, df_mouse):\n",
    "    return pd.merge(df_logs, df_mouse, on=\"session_id\", how=\"inner\")\n",
    "\n",
    "def preprocess_data(df, label_col=\"label\"):\n",
    "    df = df.dropna()\n",
    "    X = df.drop([\"session_id\", label_col], axis=1, errors=\"ignore\")\n",
    "    y = df[label_col] if label_col in df.columns else None\n",
    "    if y is not None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        return X_scaled\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory structure for Phase 1\n",
    "    # dataset/\n",
    "    # └── phase1/\n",
    "    #     ├── annotations/\n",
    "    #     │   └── humans_and_moderate_bots/\n",
    "    #     │       ├── train\n",
    "    #     │       └── test\n",
    "    #     └── data/\n",
    "    #         ├── web_logs/\n",
    "    #         │   ├── human/ (multiple log files: access_1.log, access_2.log, ...)\n",
    "    #         │   └── bots/  (access_advanced_bots.log, access_moderate_bots.log)\n",
    "    #         └── mouse_movements/\n",
    "    #             └── humans_and_moderate_bots/\n",
    "    #                 ├── <session_id1>/mouse_movements.json\n",
    "    #                 ├── <session_id2>/mouse_movements.json\n",
    "    #                 └── ...\n",
    "    \n",
    "    phase1_dir = \"web_bot_detection_dataset/phase1\"\n",
    "    # Change this to the appropriate mouse movements folder (e.g., humans_and_moderate_bots)\n",
    "    mouse_dir = os.path.join(phase1_dir, \"data\", \"mouse_movements\", \"humans_and_moderate_bots\")\n",
    "    # Annotations file for training\n",
    "    annotations_file = os.path.join(phase1_dir, \"annotations\", \"humans_and_moderate_bots\", \"train\")\n",
    "    \n",
    "    df_logs = extract_log_features(phase1_dir)\n",
    "    df_mouse = extract_mouse_features(mouse_dir)\n",
    "    df_merged = merge_features(df_logs, df_mouse)\n",
    "    \n",
    "    if os.path.exists(annotations_file):\n",
    "        df_labels = pd.read_csv(annotations_file, sep=\" \", header=None, names=[\"session_id\", \"label\"])\n",
    "        df_merged = pd.merge(df_merged, df_labels, on=\"session_id\", how=\"inner\")\n",
    "    \n",
    "    if \"label\" in df_merged.columns:\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(df_merged, label_col=\"label\")\n",
    "    else:\n",
    "        X_scaled = preprocess_data(df_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BotDetectionFeatureExtractor:\n",
    "    def __init__(self, dataset_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor with dataset path\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Root path of the dataset\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "    \n",
    "    def parse_custom_log(self, log_file: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parse custom log format with precise matching\n",
    "        \n",
    "        Args:\n",
    "            log_file (str): Path to log file\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Parsed log data\n",
    "        \"\"\"\n",
    "        # Precise log pattern matching the specific format\n",
    "        log_pattern = re.compile(\n",
    "            r'^- - \\[(?P<timestamp>[^\\]]+)\\] '\n",
    "            r'\"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>\\S+)\" '\n",
    "            r'(?P<status>\\d+) (?P<bytes>\\d+) '\n",
    "            r'\"(?P<referrer>[^\"]*)\" '\n",
    "            r'(?P<session_id>\\S+) '\n",
    "            r'\"(?P<user_agent>[^\"]*)\"'\n",
    "        )\n",
    "        \n",
    "        logs = []\n",
    "        try:\n",
    "            with open(log_file, 'r') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    match = log_pattern.match(line)\n",
    "                    \n",
    "                    if match:\n",
    "                        log_entry = match.groupdict()\n",
    "                        logs.append(log_entry)\n",
    "                    else:\n",
    "                        # Handle alternative format with '-' as user agent\n",
    "                        alt_pattern = re.compile(\n",
    "                            r'^- - \\[(?P<timestamp>[^\\]]+)\\] '\n",
    "                            r'\"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>\\S+)\" '\n",
    "                            r'(?P<status>\\d+) (?P<bytes>\\d+) '\n",
    "                            r'\"(?P<referrer>[^\"]*)\" '\n",
    "                            r'- '\n",
    "                            r'\"?(?P<user_agent>[^\"]*)\"?'\n",
    "                        )\n",
    "                        alt_match = alt_pattern.match(line)\n",
    "                        \n",
    "                        if alt_match:\n",
    "                            log_entry = alt_match.groupdict()\n",
    "                            logs.append(log_entry)\n",
    "                        else:\n",
    "                            logger.warning(f\"Skipping line {line_num} in {log_file}: {line}\")\n",
    "            \n",
    "            if not logs:\n",
    "                logger.error(f\"No logs parsed from file: {log_file}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.DataFrame(logs)\n",
    "            \n",
    "            # Timestamp parsing\n",
    "            try:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S %z')\n",
    "            except ValueError:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing log file {log_file}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    def process_dataset(self, phase: str = 'phase1') -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Process entire dataset and extract features with robust error handling\n",
    "        \n",
    "        Args:\n",
    "            phase (str): Dataset phase to process\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, List[Dict[str, Any]]]: Extracted features for different classes\n",
    "        \"\"\"\n",
    "        phase_path = os.path.join(self.dataset_path, phase)\n",
    "        features = {\n",
    "            'human': [],\n",
    "            'moderate_bots': [],\n",
    "            'advanced_bots': []\n",
    "        }\n",
    "        \n",
    "        # More flexible path handling\n",
    "        try:\n",
    "            web_logs_path = os.path.join(phase_path, 'data', 'web_logs')\n",
    "            mouse_movements_path = os.path.join(phase_path, 'data', 'mouse_movements')\n",
    "            \n",
    "            # Process human logs\n",
    "            human_log_dir = os.path.join(web_logs_path, 'humans')\n",
    "            if not os.path.exists(human_log_dir):\n",
    "                logger.error(f\"Human log directory not found: {human_log_dir}\")\n",
    "                return features\n",
    "            \n",
    "            for log_file in os.listdir(human_log_dir):\n",
    "                log_path = os.path.join(human_log_dir, log_file)\n",
    "                log_df = self.parse_custom_log(log_path)\n",
    "                \n",
    "                if log_df.empty:\n",
    "                    logger.warning(f\"Skipping empty log file: {log_path}\")\n",
    "                    continue\n",
    "                \n",
    "                log_features = self.extract_log_features(log_df)\n",
    "                \n",
    "                # Find corresponding mouse movement file\n",
    "                mouse_file = self._find_mouse_movement_file(mouse_movements_path, log_file)\n",
    "                if mouse_file:\n",
    "                    mouse_features = self.extract_mouse_movement_features(mouse_file)\n",
    "                    combined_features = {**log_features, **mouse_features, 'label': 0}  # 0 for human\n",
    "                    features['human'].append(combined_features)\n",
    "            \n",
    "            # Process moderate bot logs\n",
    "            bot_log_dir = os.path.join(web_logs_path, 'bots')\n",
    "            if not os.path.exists(bot_log_dir):\n",
    "                logger.error(f\"Bot log directory not found: {bot_log_dir}\")\n",
    "                return features\n",
    "            \n",
    "            for log_file in os.listdir(bot_log_dir):\n",
    "                if 'moderate' in log_file.lower():\n",
    "                    log_path = os.path.join(bot_log_dir, log_file)\n",
    "                    log_df = self.parse_custom_log(log_path)\n",
    "                    \n",
    "                    if log_df.empty:\n",
    "                        logger.warning(f\"Skipping empty log file: {log_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    log_features = self.extract_log_features(log_df)\n",
    "                    \n",
    "                    # Find corresponding mouse movement file\n",
    "                    mouse_file = self._find_mouse_movement_file(mouse_movements_path, log_file)\n",
    "                    if mouse_file:\n",
    "                        mouse_features = self.extract_mouse_movement_features(mouse_file)\n",
    "                        combined_features = {**log_features, **mouse_features, 'label': 1}  # 1 for moderate bot\n",
    "                        features['moderate_bots'].append(combined_features)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing dataset: {e}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _find_mouse_movement_file(self, mouse_movements_path: str, log_file: str) -> str:\n",
    "        \"\"\"\n",
    "        Find corresponding mouse movement file\n",
    "        \n",
    "        Args:\n",
    "            mouse_movements_path (str): Path to mouse movements directory\n",
    "            log_file (str): Log filename to match\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to mouse movement file or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for session_dir in os.listdir(mouse_movements_path):\n",
    "                mouse_file = os.path.join(mouse_movements_path, session_dir, 'mouse_movements.json')\n",
    "                if os.path.exists(mouse_file):\n",
    "                    return mouse_file\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding mouse movement file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def extract_log_features(self, log_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive features from log DataFrame\n",
    "        Simplified version with more robust error handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(log_df) == 0:\n",
    "                return {}\n",
    "            \n",
    "            # Basic features with error handling\n",
    "            total_requests = len(log_df)\n",
    "            total_bytes = log_df['bytes'].astype(int).sum() if 'bytes' in log_df.columns else 0\n",
    "            \n",
    "            # Timing features\n",
    "            if 'timestamp' in log_df.columns:\n",
    "                session_duration = (log_df['timestamp'].max() - log_df['timestamp'].min()).total_seconds()\n",
    "                browsing_speed = total_requests / (session_duration + 1e-10)\n",
    "            else:\n",
    "                session_duration = 0\n",
    "                browsing_speed = 0\n",
    "            \n",
    "            return {\n",
    "                'total_requests': total_requests,\n",
    "                'total_session_bytes': total_bytes,\n",
    "                'session_duration': session_duration,\n",
    "                'browsing_speed': browsing_speed\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting log features: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def extract_mouse_movement_features(self, mouse_movement_file: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Simplified mouse movement feature extraction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(mouse_movement_file, 'r') as f:\n",
    "                mouse_data = json.load(f)\n",
    "            \n",
    "            mouse_df = pd.DataFrame(mouse_data)\n",
    "            \n",
    "            return {\n",
    "                'total_mouse_movements': len(mouse_df)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing mouse movement file {mouse_movement_file}: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def prepare_training_data(self, features: Dict[str, List[Dict[str, Any]]]):\n",
    "        \"\"\"\n",
    "        Prepare features and labels for machine learning with additional error handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Combine features from different classes\n",
    "            all_features = features['human'] + features['moderate_bots']\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(all_features)\n",
    "            \n",
    "            # Handle missing data\n",
    "            df = df.fillna(0)\n",
    "            \n",
    "            # Separate features and labels\n",
    "            X = df.drop('label', axis=1)\n",
    "            y = df['label']\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preparing training data: {e}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "def train_bot_detection_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier for bot detection\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize and train model\n",
    "        rf_classifier = RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = rf_classifier.predict(X_test)\n",
    "        \n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Human', 'Moderate Bot']))\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        return rf_classifier\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training bot detection model: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Set your dataset path\n",
    "    dataset_path = 'web_bot_detection_dataset'\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    extractor = BotDetectionFeatureExtractor(dataset_path)\n",
    "    \n",
    "    # Process dataset and extract features\n",
    "    features = extractor.process_dataset('phase1')\n",
    "    print(features)\n",
    "    \n",
    "    # Validate features\n",
    "    # if not features['human'] and not features['moderate_bots']:\n",
    "    #     logger.error(\"No features extracted. Check dataset path and log files.\")\n",
    "    #     return\n",
    "    \n",
    "    # # Prepare training data\n",
    "    # X_train, X_test, y_train, y_test = extractor.prepare_training_data(features)\n",
    "    \n",
    "    # # Validate training data\n",
    "    # if X_train is None:\n",
    "    #     logger.error(\"Failed to prepare training data.\")\n",
    "    #     return\n",
    "    \n",
    "    # # Train bot detection model\n",
    "    # model = train_bot_detection_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_custom_log(log_file: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parse custom log format with precise matching\n",
    "        \n",
    "        Args:\n",
    "            log_file (str): Path to log file\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Parsed log data\n",
    "        \"\"\"\n",
    "        # Precise log pattern matching the specific format\n",
    "        log_pattern = re.compile(\n",
    "            r'^- - \\[(?P<timestamp>[^\\]]+)\\] '\n",
    "            r'\"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>\\S+)\" '\n",
    "            r'(?P<status>\\d+) (?P<bytes>\\d+) '\n",
    "            r'\"(?P<referrer>[^\"]*)\" '\n",
    "            r'(?P<session_id>\\S+) '\n",
    "            r'\"(?P<user_agent>[^\"]*)\"'\n",
    "        )\n",
    "        \n",
    "        logs = []\n",
    "        try:\n",
    "            with open(log_file, 'r') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    match = log_pattern.match(line)\n",
    "                    \n",
    "                    if match:\n",
    "                        log_entry = match.groupdict()\n",
    "                        logs.append(log_entry)\n",
    "                    else:\n",
    "                        # Handle alternative format with '-' as user agent\n",
    "                        alt_pattern = re.compile(\n",
    "                            r'^- - \\[(?P<timestamp>[^\\]]+)\\] '\n",
    "                            r'\"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>\\S+)\" '\n",
    "                            r'(?P<status>\\d+) (?P<bytes>\\d+) '\n",
    "                            r'\"(?P<referrer>[^\"]*)\" '\n",
    "                            r'- '\n",
    "                            r'\"?(?P<user_agent>[^\"]*)\"?'\n",
    "                        )\n",
    "                        alt_match = alt_pattern.match(line)\n",
    "                        \n",
    "                        if alt_match:\n",
    "                            log_entry = alt_match.groupdict()\n",
    "                            logs.append(log_entry)\n",
    "                        else:\n",
    "                            logger.warning(f\"Skipping line {line_num} in {log_file}: {line}\")\n",
    "            \n",
    "            if not logs:\n",
    "                logger.error(f\"No logs parsed from file: {log_file}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.DataFrame(logs)\n",
    "            \n",
    "            # Timestamp parsing\n",
    "            try:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S %z')\n",
    "            except ValueError:\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing log file {log_file}: {e}\")\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_log_features(log_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive features from log DataFrame\n",
    "        Simplified version with more robust error handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(log_df) == 0:\n",
    "                return {}\n",
    "            \n",
    "            # Basic features with error handling\n",
    "            total_requests = len(log_df)\n",
    "            total_bytes = log_df['bytes'].astype(int).sum() if 'bytes' in log_df.columns else 0\n",
    "            \n",
    "            # Timing features\n",
    "            if 'timestamp' in log_df.columns:\n",
    "                session_duration = (log_df['timestamp'].max() - log_df['timestamp'].min()).total_seconds()\n",
    "                browsing_speed = total_requests / (session_duration + 1e-10)\n",
    "            else:\n",
    "                session_duration = 0\n",
    "                browsing_speed = 0\n",
    "            \n",
    "            return {\n",
    "                'total_requests': total_requests,\n",
    "                'total_session_bytes': total_bytes,\n",
    "                'session_duration': session_duration,\n",
    "                'browsing_speed': browsing_speed\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting log features: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 19:27:01,339 - WARNING - Skipping line 1286 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:10:27:10 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,348 - WARNING - Skipping line 4002 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:15:42:45 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,350 - WARNING - Skipping line 4562 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:15:49:52 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,354 - WARNING - Skipping line 5350 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:15:55:07 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,357 - WARNING - Skipping line 5875 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:02:06 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,359 - WARNING - Skipping line 6197 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:06:17 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,361 - WARNING - Skipping line 6890 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:15:23 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,363 - WARNING - Skipping line 7238 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:19:55 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,365 - WARNING - Skipping line 7762 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:26:57 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,366 - WARNING - Skipping line 7958 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:29:26 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,367 - WARNING - Skipping line 8106 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:31:22 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,370 - WARNING - Skipping line 8597 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:37:42 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,371 - WARNING - Skipping line 8788 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:40:19 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,374 - WARNING - Skipping line 9167 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:45:26 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,376 - WARNING - Skipping line 10038 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:16:57:04 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,378 - WARNING - Skipping line 10405 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:17:01:55 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,380 - WARNING - Skipping line 10903 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:17:08:22 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,381 - WARNING - Skipping line 11085 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:17:10:48 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,382 - WARNING - Skipping line 11435 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:17:15:41 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,390 - WARNING - Skipping line 13554 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:19:17:45 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,393 - WARNING - Skipping line 14026 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:19:29:39 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,395 - WARNING - Skipping line 14262 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:19:35:38 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,396 - WARNING - Skipping line 14493 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:19:41:29 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,399 - WARNING - Skipping line 15426 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:20:04:50 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,400 - WARNING - Skipping line 15665 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:20:11:02 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,402 - WARNING - Skipping line 16357 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:20:28:37 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,407 - WARNING - Skipping line 17580 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:20:59:59 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,409 - WARNING - Skipping line 17819 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:06:01 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,411 - WARNING - Skipping line 18269 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:17:10 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,412 - WARNING - Skipping line 18749 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:29:07 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,414 - WARNING - Skipping line 18981 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:34:49 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,415 - WARNING - Skipping line 19208 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:40:22 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,416 - WARNING - Skipping line 19439 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:46:18 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,418 - WARNING - Skipping line 19927 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:21:58:32 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,420 - WARNING - Skipping line 20180 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:22:04:52 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,421 - WARNING - Skipping line 20419 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:22:10:52 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,422 - WARNING - Skipping line 20649 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:22:16:16 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,426 - WARNING - Skipping line 21622 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:22:40:58 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,429 - WARNING - Skipping line 22081 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:22:52:22 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,432 - WARNING - Skipping line 23266 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:23:22:38 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,434 - WARNING - Skipping line 23759 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:23:35:22 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,436 - WARNING - Skipping line 23989 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:23:41:05 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,439 - WARNING - Skipping line 24686 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [29/Oct/2019:23:58:55 +0000] \"-\" 408 156 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,441 - WARNING - Skipping line 24928 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [30/Oct/2019:00:04:54 +0000] \"-\" 408 1511 \"-\" - \"-\"\n",
      "2025-03-27 19:27:01,442 - WARNING - Skipping line 25143 in web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log: - - [30/Oct/2019:00:10:47 +0000] \"-\" 408 1511 \"-\" - \"-\"\n"
     ]
    }
   ],
   "source": [
    "temp = parse_custom_log(log_file=\"web_bot_detection_dataset/phase1/data/web_logs/humans/access_1.log\")\n",
    "temp2 =extract_log_features(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_requests': 25101,\n",
       " 'total_session_bytes': np.int64(16094655),\n",
       " 'session_duration': 72912.0,\n",
       " 'browsing_speed': 0.3442643186306776}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>protocol</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes</th>\n",
       "      <th>referrer</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-10-29 09:17:58+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-10-29 09:18:00+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-10-29 09:18:02+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-10-29 09:18:04+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-10-29 09:18:06+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-10-29 09:18:06+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-10-29 09:18:06+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/content/computer_security.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>1574</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-10-29 09:18:06+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-10-29 09:18:06+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/favicon.ico</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>1940</td>\n",
       "      <td>https://160.40.52.164/content/computer_securit...</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-10-29 09:18:08+00:00</td>\n",
       "      <td>POST</td>\n",
       "      <td>/storage/store_sess_total_mousemv_db.php</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>449</td>\n",
       "      <td>https://160.40.52.164/content/computer_securit...</td>\n",
       "      <td>g2gh9qmk9krld14h5uojlg7g10</td>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp method                                       url  \\\n",
       "10 2019-10-29 09:17:58+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "11 2019-10-29 09:18:00+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "12 2019-10-29 09:18:02+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "13 2019-10-29 09:18:04+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "14 2019-10-29 09:18:06+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "15 2019-10-29 09:18:06+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "16 2019-10-29 09:18:06+00:00    GET            /content/computer_security.php   \n",
       "17 2019-10-29 09:18:06+00:00   POST             /storage/store_mousemv_db.php   \n",
       "18 2019-10-29 09:18:06+00:00    GET                              /favicon.ico   \n",
       "19 2019-10-29 09:18:08+00:00   POST  /storage/store_sess_total_mousemv_db.php   \n",
       "\n",
       "    protocol status bytes                                           referrer  \\\n",
       "10  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "11  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "12  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "13  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "14  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "15  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "16  HTTP/1.1    200  1574                             https://160.40.52.164/   \n",
       "17  HTTP/1.1    200   449                             https://160.40.52.164/   \n",
       "18  HTTP/1.1    200  1940  https://160.40.52.164/content/computer_securit...   \n",
       "19  HTTP/1.1    200   449  https://160.40.52.164/content/computer_securit...   \n",
       "\n",
       "                    session_id  \\\n",
       "10  g2gh9qmk9krld14h5uojlg7g10   \n",
       "11  g2gh9qmk9krld14h5uojlg7g10   \n",
       "12  g2gh9qmk9krld14h5uojlg7g10   \n",
       "13  g2gh9qmk9krld14h5uojlg7g10   \n",
       "14  g2gh9qmk9krld14h5uojlg7g10   \n",
       "15  g2gh9qmk9krld14h5uojlg7g10   \n",
       "16  g2gh9qmk9krld14h5uojlg7g10   \n",
       "17  g2gh9qmk9krld14h5uojlg7g10   \n",
       "18  g2gh9qmk9krld14h5uojlg7g10   \n",
       "19  g2gh9qmk9krld14h5uojlg7g10   \n",
       "\n",
       "                                           user_agent  \n",
       "10  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "11  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "12  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "13  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "14  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "15  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "16  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "17  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "18  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  \n",
       "19  Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mouse.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
